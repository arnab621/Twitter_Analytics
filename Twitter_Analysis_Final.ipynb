{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.collocations import *\n",
    "import nltk.draw\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#for ipython\n",
    "%matplotlib inline\n",
    "\n",
    "#pre-processing tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allFiles = glob.glob(\"Fintech/Payment/Fintech_*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df0 = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df0)\n",
    "df = pd.concat(list_)\n",
    "df['date'] = pd.to_datetime(df.Timestamp.map(lambda x: datetime.strptime(x,'%H:%M %p - %d %b %Y')))\n",
    "#df['month_yr'] = df.date.map(lambda x: x.strftime('%Y-%m'))\n",
    "#df.groupby(['month_yr']).agg(['count']).Tweet_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv(\"Fintech/Payment/currencycloud -recruit -recruitment -career -job -hiring_2015-12-31_2015-01-01_tweets.csv\")\n",
    "#df2 = pd.read_csv(\"Fintech/Payment/getmoven -recruit -recruitment -career -job -hiring_2015-12-31_2015-01-01_tweets.csv\")\n",
    "#df3 = pd.read_csv(\"Fintech/Payment/samsungpay -recruit -recruitment -career -job -hiring_2015-12-31_2015-01-01_tweets.csv\")\n",
    "#df4 = pd.read_csv(\"Fintech/Payment/transferwise -recruit -recruitment -career -job -hiring_2015-12-31_2015-01-01_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.append(df4, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.Timestamp.map(lambda x: datetime.strptime(x,'%H:%M %p - %d %b %Y')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfQ4 = df[df.date >= datetime(2015, 10, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14410"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfQ4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df[df.date < datetime(2015, 4, 1)]\n",
    "df2 = df[(df.date < datetime(2015, 7, 1)) & (df.date >= datetime(2015, 4, 1))]\n",
    "\n",
    "df3 = df[(df.date < datetime(2015, 10, 1)) & (df.date >= datetime(2015, 7, 1))]\n",
    "df4 = df[df.date >= datetime(2015, 10, 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfn =df4.copy()\n",
    "df0 = dfn[dfn.Tweet_Text.str.contains(\"http\") == False]\n",
    "df_new = df0[df0.Tweet_Text.str.contains(\".com\") == False]\n",
    "df_final = df_new.drop_duplicates('Tweet_Text')\n",
    "#len(df_new.Tweet_Text.drop_duplicates())\n",
    "df_final.to_csv('Fintech/Virtual_Bank/Fintech_VirtualBank_Q4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_final.to_csv('Fintech/Lending/Fintech_Lending_Q2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df01 = pd.read_csv('applepay_tweets1.csv')\n",
    "#df02 = pd.read_csv('applepay_tweets2.csv')\n",
    "#df = df01.append(df02, ignore_index=True)\n",
    "#df = pd.read_csv(\"Fintech/Payment/bitcoin OR cryptocurrency -recruit -recruitment -career -job -hiring_tweets.csv\")\n",
    "#df = pd.read_csv('Fintech/Real_Estate/Fintech_RealEstate_Q3.csv')\n",
    "#df = pd.read_csv('bitcoin OR cryptocurrency -recruit -recruitment -career -job -hiring_tweets.csv')\n",
    "#df = pd.read_csv('yodlee -recruit -recruitment -career -job -hiring_tweets.csv')\n",
    "df=dfQ4\n",
    "df0 = df[df.Tweet_Text.str.contains(\"http\") == False]\n",
    "df1 = df0[df0.Tweet_Text.str.contains(\".com\") == False]\n",
    "\n",
    "tweets = list(df1.Tweet_Text.drop_duplicates())\n",
    "\n",
    "#tweet_processor = TweetPreprocessor()\n",
    "#tknzr = TweetTokenizer()\n",
    "#nltk.download('stopwords')\n",
    "#stop = stopwords.words('english')\n",
    "#stop += ['<hashtag>', '<url>', '<allcaps>', '<number>', '<user>', '<repeat>', '<elong>', 'websummit', 'battery', 'song']\n",
    "#outtweets = []\n",
    "#for tweet in tweets:\n",
    "#    parts = tknzr.tokenize(tweet_processor.preprocess(tweet))\n",
    "#    clean = [i.encode('ascii', 'ignore') for i in parts if i not in stop]\n",
    "#    outtweets.append(clean)\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(str(tweets).translate(None, string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4345"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop += ['rt','im','thats', 'dont', '.' , \\\n",
    "        ',', '?', '!']\n",
    "stop += ['real estate', 'apple pay']\n",
    "stop += ['paypal', 'bitcoin', 'allybank', 'sainsbury', 'tesco', 'metro', 'kabbage', 'zopa', 'lendzoan', 'funding circle', 'mint']\n",
    "\n",
    "stop += ['allybank', 'bank', 'synchrony', 'bank5connect', 'synchronybank', 'rabodirect',  \\\n",
    "        'ally']\n",
    "tweet_texts_processed = [i.lower().encode('ascii', 'ignore') for i in tokens if i.lower() not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = nltk.pos_tag(tweet_texts_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nouns = [word for word,pos in tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "#downcased = [x.lower() for x in nouns]\n",
    "#joined = \" \".join(downcased).encode('ascii', 'ignore')\n",
    "#into_string = str(nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['account',  'apple', 'service', 'mobile', 'money', 'app', 'safe', 'machine', 'tech', 'smart', 'online',\\\n",
    "          'trust', 'technology', 'payment', 'card', 'secure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#textNLTK=nltk.Text.cont(nouns)\n",
    "textNLTK=nltk.text.ContextIndex(nouns)\n",
    "x.append(textNLTK.similar_words(\"apple\")) #Find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z=[{x:textNLTK.similar_words(\"x\")} for x in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_nouns=nltk.FreqDist(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_nouns.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_nouns.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samsung android account cant use able trying bill phone used want way\n",
      "need option let using card tried payment get\n"
     ]
    }
   ],
   "source": [
    "textNLTK=nltk.Text(tweet_texts_processed)\n",
    "textNLTK.similar(\"apple\") #Find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation = re.compile(r'[-.?!,\":;()|0-9]')\n",
    "tweet_texts_processed = [punctuation.sub(\"\", word) for word in tweet_texts_processed] \n",
    "#Create your bigrams\n",
    "bgs = nltk.bigrams(tweet_texts_processed)\n",
    "tgs = nltk.trigrams(tweet_texts_processed)\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist_bgs = nltk.FreqDist(bgs)\n",
    "fdist_tgs = nltk.FreqDist(tgs)\n",
    "#for k,v in fdist.items():\n",
    "#    print k,v\n",
    "fdist_bgs.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "finderBi = BigramCollocationFinder.from_words(tweet_texts_processed)\n",
    "finderTri = TrigramCollocationFinder.from_words(tweet_texts_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finderBi.nbest(bigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finderTri.nbest(trigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finderBi.apply_freq_filter(2) #filter out based on total occurrence of the word\n",
    "finderTri.apply_freq_filter(2) #filter out based on total occurrence of the word\n",
    "\n",
    "finderBi.apply_word_filter(lambda w: w in ('I', 'me', '(', ')', '/', ':', '*', '\"', 're', '?', ';', '-', '$'))\n",
    "finderTri.apply_word_filter(lambda w: w in ('I', 'me', '(', ')', '/', ':', '*', '\"', 're', '?', ';', '-', '$'))\n",
    "\n",
    "scoredBi = finderBi.score_ngrams(bigram_measures.raw_freq)\n",
    "scoredTri = finderBi.score_ngrams(trigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(bigram for bigram, score in scoredBi) == set(nltk.bigrams(tweet_texts_processed))\n",
    "sorted(finderBi.nbest(bigram_measures.raw_freq, 80),  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(trigram for trigram, score in scoredTri) == set(nltk.trigrams(tweet_texts_processed))\n",
    "sorted(finderTri.nbest(trigram_measures.raw_freq, 100), reverse=True)\n",
    "#finderTri.nbest(trigram_measures.raw_freq, 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "                                       ##### Rough work #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create your bigrams\n",
    "bgs = nltk.bigrams(tweet_texts_processed)\n",
    "tgs = nltk.trigrams(tweet_texts_processed)\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist_bgs = nltk.FreqDist(bgs)\n",
    "fdist_tgs = nltk.FreqDist(tgs)\n",
    "#for k,v in fdist.items():\n",
    "#    print k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fdist_bgs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textNLTK=nltk.Text(tweet_texts_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textNLTK.collocations(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_bgs.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_tgs.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outtweets=nltk.Text(tokens) #convert to NLTK class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "tweet_texts_processed = [i.encode('ascii', 'ignore') for i in outtweets if i not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removing numbers and punctuation using regexp\n",
    "\n",
    "punctuation = re.compile(r'[-.?!,\":;()|0-9]')\n",
    "tweet_texts_processed = [punctuation.sub(\"\", word) for word in tweet_texts_processed]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textNLTK=nltk.Text(tweet_texts_processed)\n",
    "textNLTK.similar(\"reward\") #Find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist1 = nltk.FreqDist(textNLTK)\n",
    "print(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist1.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist1.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "finderBi = BigramCollocationFinder.from_words(tweet_texts_processed)\n",
    "finderTri = TrigramCollocationFinder.from_words(tweet_texts_processed)\n",
    "\n",
    "finderBi.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finderBi.apply_freq_filter(2) #filter out based on total occurrence of the word\n",
    "finderTri.apply_freq_filter(2) #filter out based on total occurrence of the word\n",
    "\n",
    "finderBi.apply_word_filter(lambda w: w in ('I', 'me', '(', ')', '/', ':', '*', '\"', 're', '?', ';', '-', '$'))\n",
    "finderTri.apply_word_filter(lambda w: w in ('I', 'me', '(', ')', '/', ':', '*', '\"', 're', '?', ';', '-', '$'))\n",
    "\n",
    "scoredBi = finderBi.score_ngrams(bigram_measures.raw_freq)\n",
    "scoredTri = finderBi.score_ngrams(trigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(bigram for bigram, score in scoredBi) == set(nltk.bigrams(tweet_texts_processed))\n",
    "sorted(finderBi.nbest(bigram_measures.raw_freq, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(trigram for trigram, score in scoredTri) == set(nltk.trigrams(tweets))\n",
    "sorted(finderTri.nbest(trigram_measures.raw_freq, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#POS tagging\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "sentence = \"Michael Jackson likes to eat at McDonalds\"\n",
    "tagged_sent = pos_tag(tweet_texts_processed)\n",
    "#(sentence.split())\n",
    "# [('Michael', 'NNP'), ('Jackson', 'NNP'), ('likes', 'VBZ'), ('to', 'TO'), ('eat', 'VB'), ('at', 'IN'), ('McDonalds', 'NNP')]\n",
    "\n",
    "propernouns = [word for word,pos in tagged_sent if pos == 'NNP']\n",
    "# ['Michael','Jackson', 'McDonalds']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
